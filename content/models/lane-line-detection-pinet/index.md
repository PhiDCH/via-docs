---
title: "Ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng - PiNet"
description: "Ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng l√† b√†i to√°n r·∫•t quan tr·ªçng khi x√¢y d·ª±ng xe t·ª± h√†nh. M√¥ h√¨nh ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng d·ª±a tr√™n PiNet cho t·ªëc ƒë·ªô cao v√† ƒë·ªô ch√≠nh x√°c t∆∞∆°ng ƒë·ªëi t·ªët cho c√°c ·ª©ng d·ª•ng xe t·ª± h√†nh c·ªßa VIA."
image: pinet.png
tags: ["Lane line"]
date: 2021-05-05
author: "Hu·ª≥nh ƒê·ª©c"
authorUrl: "https://github.com/ducnguyenhuynh/"
sourceCodeUrl: "https://github.com/ducnguyenhuynh/via-line-detection"
notebookUrl: "https://colab.research.google.com/drive/1Bn-9WOgkQuYMX0fIYu5ChkAQPQyHqhEo"
accuracy: "0.xx mAP"
fps: "xx FPS on NVIDIA RTX 2070"
version: "1.0"
---


## I. Th√¥ng tin m√¥ h√¨nh

### 1. Gi·ªõi thi·ªáu

Ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng l√† b√†i to√°n r·∫•t quan tr·ªçng khi x√¢y d·ª±ng xe t·ª± h√†nh. C√≥ nhi·ªÅu c√°ch ƒë·ªÉ m√°y t√≠nh c√≥ th·ªÉ ph√°t hi·ªán l√†n ƒë∆∞·ªùng t·ª´ m·ªôt t·∫•m h√¨nh. C√°ch c∆° b·∫£n nh·∫•t ta c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n x·ª≠ l√Ω ·∫£nh ƒë·ªÉ ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng th√¥ng qua m√†u s·∫Øc hay c√°c thu·∫≠t to√°n ph√°t hi·ªán c·∫°nh nh∆∞ Canny. Cao si√™u h∆°n ta c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c m√¥ h√¨nh m√°y h·ªçc ƒë·ªÉ ph√¢n ƒëo·∫°n hay ph√°t hi·ªán v·∫°ch k·∫ª. M√¥ h√¨nh ƒë∆∞·ª£c gi·ªõi thi·ªáu ·ªü ƒë√¢y ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n ki·∫øn tr√∫c **PiNet**.

**Input:** H√¨nh ·∫£nh RGB c√≥ k√≠ch th∆∞·ªõc 512x256.

**Output:** T·∫≠p t·ªça ƒë·ªô c√°c ƒëi·ªÉm (pixel) tr√™n line ƒë∆∞·ªùng.

M√¥ h√¨nh d·ª±a tr√™n key points estimation v√† instance segmentation approach. PINet bao g·ªìm m·ªôt v√†i hourglass networks ƒë∆∞·ª£c x·∫øp ch·ªìng l√™n nhau v√† ƒë∆∞·ª£c train ƒë·ªìng th·ªùi, v√¨ v·∫≠y k√≠ch th∆∞·ªõc c·ªßa m√¥ h√¨nh c√≥ th·ªÉ ƒë∆∞·ª£c thay ƒë·ªïi d·ª±a tr√™n s·ª©c m·∫°nh t√≠nh to√°n ph·∫ßn c·ª©ng.

![M√¥ h√¨nh m·∫°ng PINet](pinet.png)

**Paper:** [Key Points Estimation and Point Instance Segmentation Approach for Lane Detection Edit social preview. Yeongmin Ko ‚Ä¢ Younkwan Lee ‚Ä¢ Shoaib Azam ‚Ä¢ Farzeen Munir ‚Ä¢ Moongu Jeon ‚Ä¢ Witold Pedrycz](https://arxiv.org/abs/2002.06604).

### 2. K·∫øt qu·∫£

**Demo:** 

![Ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng v·ªõi PINet](https://github.com/ducnguyenhuynh/via-line-detection/raw/main/images/result_demo.gif)

**ƒê·ªô ƒëo:** D·ª±a tr√™n ƒë·ªô ch√≠nh x√°c c·ªßa c√°c pixel tr√™n ground truth

**B·ªô d·ªØ li·ªáu ki·ªÉm tra:** Do t·∫≠p d·ªØ li·ªáu c√≤n h·∫°n ch·∫ø n√™n m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c ƒë√°nh gi√° d·ª±a tr√™n t·∫≠p validation

**ƒê·ªô ch√≠nh x√°c v√† T·ªëc ƒë·ªô:** Accuracy tr√™n t·∫≠p val l√† 0.99. FPS ƒë·∫°t 35-40 (Ph·∫ßn c·ª©ng I5-9300H, GeForce GTX 1050) 

### 3. H·∫°n ch·∫ø v√† h∆∞·ªõng c·∫£i ti·∫øn

**H·∫°n ch·∫ø:**

<!-- - H·∫°n ch·∫ø c·ªßa m√¥ h√¨nh l√† g√¨ ? M·∫•t c√¢n b·∫±ng d·ªØ li·ªáu? T·ªëc ƒë·ªô th·∫•p? ƒê·ªô ch√≠nh x√°c ch∆∞a cao? -->
- Updating

**H∆∞·ªõng c·∫£i ti·∫øn:**
- Convert sang TensorRT ƒë·ªÉ deploy tr√™n c√°c jetson
- Test m√¥ h√¨nh tr√™n CPU
- ·ª®ng d·ª•ng c·ªßa m√¥ h√¨nh: Autopilot

## II. Ch·∫°y th·ª≠ v√† t√≠ch h·ª£p m√¥ h√¨nh

### 1. Ch·∫°y m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr√™n ·∫£nh

```
```

**K·∫øt qu·∫£:**

TODO

### 2. Ch·∫°y m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán tr√™n video

```
```

**K·∫øt qu·∫£:**

TODO

### 3. T√≠ch h·ª£p l√™n h·ªá th·ªëng VIA SDK

```
```

**K·∫øt qu·∫£:**

TODO


## II. Hu·∫•n luy·ªán m√¥ h√¨nh

H∆∞·ªõng d·∫´n sau s·∫Ω h∆∞·ªõng d·∫´n c√°c b·∫°n chu·∫©n b·ªã d·ªØ li·ªáu, v√† hu·∫•n luy·ªán m√¥ h√¨nh PINet cho ph√°t hi·ªán v·∫°ch k·∫ª ƒë∆∞·ªùng. C√°c b·∫°n c≈©ng c√≥ th·ªÉ ch·ªânh s·ª≠a m√£ ngu·ªìn v√† thay ƒë·ªïi b·ªô d·ªØ li·ªáu s·ª≠ d·ª•ng ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh n√†y cho c√°c m·ª•c ƒë√≠ch kh√°c nhau.

### 1. Th√¥ng tin d·ªØ li·ªáu

Chu·∫©n b·ªã d·ªØ li·ªáu v√† m·ªôt vi·ªác quan tr·ªçng v√† chi·∫øm kh√° nhi·ªÅu th·ªùi gian trong x√¢y d·ª±ng m·ªôt h·ªá th·ªëng h·ªçc m√°y.

- B·ªô d·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y ·ªü ƒë√¢u?
- Ph√¢n chia ra sao? (train/val/test)
- C√°c ƒë·∫∑c ƒëi·ªÉm b·ªô d·ªØ li·ªáu n√†y.
### 2. T·∫£i m√£ ngu·ªìn

TODO: Th√™m m·ªôt s·ªë h∆∞·ªõng d·∫´n

```python
!git clone https://github.com/ducnguyenhuynh/via-line-detection.git
```

    Cloning into 'via-line-detection'...
    remote: Enumerating objects: 902, done.[K
    remote: Counting objects: 100% (902/902), done.[K
    remote: Compressing objects: 100% (869/869), done.[K
    remote: Total 902 (delta 57), reused 869 (delta 27), pack-reused 0[K
    Receiving objects: 100% (902/902), 47.80 MiB | 42.08 MiB/s, done.
    Resolving deltas: 100% (57/57), done.


### 3. T·∫£i v√† ph√¢n chia d·ªØ li·ªáu

TODO: Th√™m m·ªôt s·ªë h∆∞·ªõng d·∫´n

```python
cd via-line-detection/
```

```python
!wget https://github.com/ducnguyenhuynh/via-line-detection/releases/download/v1.1/via-dataset-line-dectection.zip -O ./dataset.zip
!unzip dataset.zip
!mv via-data-line-detection/ dataset
!rm dataset.zip
!pip install -r requirements.txt
```

### 4 Hu·∫•n luy·ªán m√¥ h√¨nh PINet

TODO: Th√™m m·ªôt s·ªë l∆∞u √Ω, gi·∫£i th√≠ch t·∫°i sao c·∫ßn l√†m nh∆∞ v·∫≠y. Vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh n√™n ƒë∆°n gi·∫£n, ch·∫°y √≠t file, s·ª≠ d·ª•ng c√°c file c·∫•u h√¨nh .py ho·∫∑c .json ƒë·ªÉ n·∫°p c·∫•u h√¨nh hu·∫•n luy·ªán.


```python
cd src
```

    /content/via-line-detection/src

C·∫•u h√¨nh c√°c hyperparameters trong qu√° tr√¨nh train t·∫°i file parameters.py

```python
%%writefile parameters.py
#############################################################################################################
##
##  Parameters
##
#############################################################################################################
import numpy as np
import cv2

class Parameters():
    # thay ƒë·ªïi s·ªë l∆∞·ª£ng epoch ·ªü ƒë√¢y
    n_epoch = 30
    l_rate = 0.0001
    weight_decay=1e-5
    save_path = "savefile/"
    # train from scratch.
    model_path = "savefile/"
    batch_size = 8
    x_size = 512
    y_size = 256
    resize_ratio = 8
    grid_x = x_size//resize_ratio  #64
    grid_y = y_size//resize_ratio  #32
    feature_size = 4
    regression_size = 110
    mode = 2
    threshold_point = 0.75 #0.35 #0.5 #0.57 #0.64 #0.35
    threshold_instance = 0.1

    #loss function parameter
    K1 = 1.0                     #  ####################################
    K2 = 2.0
    constant_offset = 0.2
    constant_exist = 1.0 #2.0#1.0    #8
    constant_nonexist = 1.0#3.0
    constant_angle = 1.0
    constant_similarity = 1.0
    constant_attention = 0.1
    constant_alpha = 0.5 #in SGPN paper, they increase this factor by 2 every 5 epochs
    constant_beta = 0.5
    constant_l = 1.0
    constant_lane_loss = 1.0  #10  ######################################
    constant_instance_loss = 1.0

    #data loader parameter
    flip_ratio=0.6
    translation_ratio=0.6
    rotate_ratio=0.6
    noise_ratio=0.6
    intensity_ratio=0.6
    shadow_ratio=0.6
    scaling_ratio=0.2
    flip_indices=[(0,34),(1,35),(2,36),(3,37),(4,38),(5,39),(6,40),(7,41),(8,42),(9,43),(10,44),(11,45),(12,46),(13,47),(14,48),(15,49),(16,50),(17,51)
                    ,(18,52),(19,53),(20,54),(21,55),(22,56),(23,57),(24,58),(25,59),(26,60),(27,61),(28,62),(29,63),(30,64),(31,65)
                    ,(32,66),(33,67),(68,68),(69,69),(70,72),(71,73)]
    
    train_root_url="../dataset/train/"
    train_labels_root="../dataset/train/"

    test_root_url="../dataset/val/"
    test_labels_root="../dataset/val/"
    
    # test parameter
    color = [(0,0,0), (255,0,0), (0,255,0),(0,0,255),(255,255,0),(255,0,255),(0,255,255),(255,255,255),(100,255,0),(100,0,255),(255,100,0),(0,100,255),(255,0,100),(0,255,100)]
    grid_location = np.zeros((grid_y, grid_x, 2))
    for y in range(grid_y):
        for x in range(grid_x):
            grid_location[y][x][0] = x
            grid_location[y][x][1] = y
    num_iter = 30
    threshold_RANSAC = 0.1
    ratio_inliers = 0.1

    # expand

    point_in_lane = 0
    source_points = np.float32([
    [0, y_size],
    [0, (5/9)*y_size],
    [x_size, (5/9)*y_size],
    [x_size, y_size]
    ])
    
    destination_points = np.float32([
    [0 * x_size, y_size],
    [0 * x_size, 0],
    [x_size - (0 * x), 0],
    [x_size - (0 * x), y_size]
    ])
    
    perspective_transform = cv2.getPerspectiveTransform(source_points, destination_points)
    inverse_perspective_transform = cv2.getPerspectiveTransform( destination_points, source_points)
```

    Overwriting parameters.py



Gi·ªù th√¨ train th√¥i :))))

```python
!python train.py
```

    Training
    Initializing hyper parameter
    Get dataset
    1423
    Get agent
    model parameters: 
    4056849
    Setup GPU mode
    Training loop
    epoch : 0
    step : 0
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0943, device='cuda:0')
    different instance loss:  tensor(0.0218, device='cuda:0')
    point loss
    exist loss:  tensor(0.3546, device='cuda:0')
    non-exit loss:  tensor(0.0907, device='cuda:0')
    offset loss:  tensor(0.1419, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.5981, device='cuda:0')
    epoch : 0
    step : 1
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0061, device='cuda:0')
    different instance loss:  tensor(3.4911e-06, device='cuda:0')
    point loss
    exist loss:  tensor(0.0507, device='cuda:0')
    non-exit loss:  tensor(0.0819, device='cuda:0')
    offset loss:  tensor(0.0770, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.2172, device='cuda:0')
    epoch : 0
    step : 2
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0475, device='cuda:0')
    different instance loss:  tensor(0.0068, device='cuda:0')
    point loss
    exist loss:  tensor(0.2414, device='cuda:0')
    non-exit loss:  tensor(0.0695, device='cuda:0')
    offset loss:  tensor(0.1065, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.4255, device='cuda:0')
    epoch : 0
    step : 3
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0038, device='cuda:0')
    different instance loss:  tensor(0.0040, device='cuda:0')
    point loss
    exist loss:  tensor(0.0745, device='cuda:0')
    non-exit loss:  tensor(0.0716, device='cuda:0')
    offset loss:  tensor(0.0846, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.2313, device='cuda:0')
    epoch : 0
    step : 4
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0685, device='cuda:0')
    different instance loss:  tensor(0.0691, device='cuda:0')
    point loss
    exist loss:  tensor(0.4021, device='cuda:0')
    non-exit loss:  tensor(0.0708, device='cuda:0')
    offset loss:  tensor(0.1418, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.6355, device='cuda:0')
    epoch : 0
    step : 5
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0483, device='cuda:0')
    different instance loss:  tensor(0.0267, device='cuda:0')
    point loss
    exist loss:  tensor(0.1934, device='cuda:0')
    non-exit loss:  tensor(0.0707, device='cuda:0')
    offset loss:  tensor(0.1102, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.3866, device='cuda:0')
    epoch : 0
    step : 6
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0211, device='cuda:0')
    different instance loss:  tensor(0.0054, device='cuda:0')
    point loss
    exist loss:  tensor(0.1774, device='cuda:0')
    non-exit loss:  tensor(0.0628, device='cuda:0')
    offset loss:  tensor(0.1049, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.3374, device='cuda:0')
    epoch : 0
    step : 7
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0165, device='cuda:0')
    different instance loss:  tensor(0.0045, device='cuda:0')
    point loss
    exist loss:  tensor(0.1225, device='cuda:0')
    non-exit loss:  tensor(0.0599, device='cuda:0')
    offset loss:  tensor(0.0911, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.2740, device='cuda:0')
    epoch : 0
    step : 8
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0289, device='cuda:0')
    different instance loss:  tensor(0.0136, device='cuda:0')
    point loss
    exist loss:  tensor(0.1794, device='cuda:0')
    non-exit loss:  tensor(0.0724, device='cuda:0')
    offset loss:  tensor(0.1030, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.3552, device='cuda:0')
    epoch : 0
    step : 9
    ######################################################################
    seg loss
    same instance loss:  tensor(0.0225, device='cuda:0')
    different instance loss:  tensor(0.0077, device='cuda:0')
    point loss
    exist loss:  tensor(0.1470, device='cuda:0')
    non-exit loss:  tensor(0.0466, device='cuda:0')
    offset loss:  tensor(0.0960, device='cuda:0')
    attention loss
    attention loss:  0
    --------------------------------------------------------------------
    total loss:  tensor(0.2884, device='cuda:0')
    epoch : 0
    step : 10
    ######################################################################

### 5 ƒê√°nh gi√°

``` python
!python test.py
```
    1423
    model parameters: 
    4056849
    7% 1/15 [00:00<00:02,  6.87it/s]
    31it [00:04,  7.33it/s]
``` python
!python evaluation.py
```
    [{"name":"Accuracy","value":0.9974696356275303,"order":"desc"},{"name":"FP","value":0.42309620204357173,"order":"asc"},{"name":"FN","value":0.0,"order":"asc"}]

Accuracy ƒë·∫°t 0,99 v·ªõi tr√™n t·∫≠p val

### 6 Ch·∫°y Demo

Thay ƒë·ªïi m·ªôt s·ªë ƒë∆∞·ªùng d·∫´n trong c√°c file py:
- util_hourglass.py: uncomment line 9, comment line 8
- hourglass_network.py: uncomment line 9, comment line 8
- processing_image.py: uncomment line 4, comment line 5
- util.py: uncomment line 10, comment line 9

Demo tr√™n ·∫£nh 
``` python
!python demo_line_detection.py -o image -d "images_test/2lines-00001086.jpg"
```
Demo tr√™n video

    T·∫£i video
``` python
!mkdir video
!wget https://github.com/ducnguyenhuynh/via-line-detection/releases/download/v1.0/demo.avi -O video/demo.avi
```
    --2021-04-19 14:40:52--  https://github.com/ducnguyenhuynh/via-line-detection/releases/download/v1.0/demo.avi
    Resolving github.com (github.com)... 192.30.255.112
    Connecting to github.com (github.com)|192.30.255.112|:443... connected.
    HTTP request sent, awaiting response... 302 Found
    Location: https://github-releases.githubusercontent.com/354692123/ae188000-a157-11eb-8cb8-460239f853a9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210419%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210419T144052Z&X-Amz-Expires=300&X-Amz-Signature=9eaec88e116e1c0886a6949db168f1ec76d2495d55a227ca6a383fbbf5b95c8e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=354692123&response-content-disposition=attachment%3B%20filename%3Ddemo.avi&response-content-type=application%2Foctet-stream [following]
    --2021-04-19 14:40:52--  https://github-releases.githubusercontent.com/354692123/ae188000-a157-11eb-8cb8-460239f853a9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210419%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210419T144052Z&X-Amz-Expires=300&X-Amz-Signature=9eaec88e116e1c0886a6949db168f1ec76d2495d55a227ca6a383fbbf5b95c8e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=354692123&response-content-disposition=attachment%3B%20filename%3Ddemo.avi&response-content-type=application%2Foctet-stream
    Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.110.154, 185.199.109.154, 185.199.111.154, ...
    Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.110.154|:443... connected.
    HTTP request sent, awaiting response... 200 OK
    Length: 39065134 (37M) [application/octet-stream]
    Saving to: ‚Äòvideo/demo.avi‚Äô

    video/demo.avi      100%[===================>]  37.25M  51.5MB/s    in 0.7s    

    2021-04-19 14:40:53 (51.5 MB/s) - ‚Äòvideo/demo.avi‚Äô saved [39065134/39065134]

``` python 
!python demo_line_detection.py -o video -d "video/demo.avi" -s 1
```

TODO: Sau khi hu·∫ßn luy·ªán, t·∫£i m√¥ h√¨nh v·ªÅ ra sao?
